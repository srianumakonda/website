<html>
<head>
<title>How you can figure out your teacher&#39;s age without embarrassing yourself. </title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href='https://fonts.googleapis.com/css?family=Poppins' rel='stylesheet'>
<link rel="shortcut icon" href="img/sitelogo.png">
<style>
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
body {
    margin: 200px;
    font-family: 'Poppins';
    background-color:#F5F5F5;
}
</style>
</head>

<body>

<a href="../index.html">< Back to Website</a> <br>
<a href="../blog.html">< Back to blog</a>
<h1>How you can figure out your teacher&#39;s age without embarrassing yourself</h1>
<p>An article talking about a Convolutional Neural Network I designed to predict the age, gender, and ethnicity of people</p>
<p>Date released: 1 Oct 2020 | Reading time: 11 minutes | Category: <a href="../blog.html#tech">Tech</a></p>
<hr>

<p>Remember those times when you were young and you accidentally asked your teacher how old she was? Yeah, I know, the ending didnâ€™t go that well. Your parents got mad at you since they already told you the saying of â€œnever ask a woman her age and a man his salaryâ€ ğŸ™„. But isnâ€™t it painful when you ask a question directly to someone and the only answer you get out of them is you embarrassing yourself? </p>
<p>But what if I were to tell you that there was a way to get out of this? What if you were able to just get a picture of any person in this world and figure out their age? Iâ€™ve taught AI (yes, my ğŸ’») to tell me how old my teacher is without having to ask her and have all my friends laugh at me for being an idiot. Letâ€™s talk a bit more about it. </p>

<h2>Nerding Out Season 1 Episode 101: How your Windows Vista from WWII Readâ€™s Images</h2>

<p>Iâ€™m not gonna go over what Artificial Intelligence and Deep Learning are but I did write the <a href="https://medium.com/analytics-vidhya/the-ultimate-ai-textbook-dc2cf5dfe755" target="_blank">Ultimate AI Textbook</a> that has everything (like a textbook) for you to understand all the advanced math and precalculus in ğŸ‘¶ language.</p>
<img src="https://miro.medium.com/max/700/0*nw8EjKuwOQySu6hB" >
<p>Artifical Intelligence is all about the science of making machines learn. Just think of it like this: you have a math test on vectors tomorrow and your teacher didnâ€™t even explain what vectors even are. Youâ€™re like, â€œWhat the heck? What do I do? ugh Iâ€™m gonna fail the test tomorrow and end up getting laughed by my friends ğŸ˜¦.â€ But thanks to Steve Jobs, we have a chatbot known as Siri. So, you skrt skrt and go, â€œHey Siri.â€ 5 minutes later of chatting about vectors, you realize, â€œHey! I actually have friends. Its name is Siri!â€</p>
<p>The â€œhowâ€ part of your conversation is AI. Siri was thinking like a human. Heck, itâ€™s an artificial human thatâ€™s replicating human intelligence (hence the name artifical intelligence). But letâ€™s go a bit deeper into Deep Learning.</p>
Like I said with AI, refer to my <a href="https://medium.com/analytics-vidhya/the-ultimate-ai-textbook-dc2cf5dfe755" target="_blank">Ultimate AI Textbook</a> for a better in-depth understanding of everything. Deep Learning is all about mimicking the human brain through something known as an Artifical Neural Network.
<img src="https://miro.medium.com/max/700/0*pGydGvEAkN8qWR9v.png">
<p>Iâ€™ll 100% agree with you there. This is SCARY. Like being jumpscared on Halloween is NOTHING compared to this. Imagine having to go through Wx+b every single day (ok not that scary but you get the point ğŸ˜‰)!</p>
<p>Deep Learning has 3 main layers: an input layer, a hidden layer, and an output layer. The input layer is all about data. This is where you input your data into the neural network. From there, that data goes through different types of functions such as an activation function. Basically, letâ€™s look at h[1][1] (the first neuron in the h1 layer in the image above â˜). h1 is the weighted sum of every single input in input layer i.</p>
<p>Think of it like this: letâ€™s just say that i[1] (the first neuron in layer i) has a value of 1. The weight (which is the line that connects to the neurons in h1) has a value associated with it. Letâ€™s say that the weight value for i[1] and h[1][1] is 2. Letâ€™s say that we also have a bias of 1. So, weâ€™d run this through our function Wx+b. We now just substitute the values ( 2*1 + 1 = 3) and now h[1][1] has a value of 3! Weâ€™d just continue doing this for every single neuron until weâ€™re done.</p>
<p>Just to recap, the way we get x is from the input data. For example, each neuron in the input could be data in Fahrenheit and our goal is to teach the neural network how to convert that into Celsius. The w (weight) tells us how important that neuron is. The bigger the weight, the more prominent role it plays in the neural network. For example, if I wanted a computer to read images of handwritten digits, then weâ€™d want the neurons where which store the handwritten digit to have a greater weight since thatâ€™s what we want to know.</p>
<p>After getting all the weighted sum for h[1][1], then weâ€™d run that through an activation function. An activation function tells the neural network if a particular neuron should be fired or not. The output will always be between 0 and 1 (since the margin is small the computer will get higher accuracy). The output will vary based on the function you use. For example, the step function only outputs a 0 or 1 if the output is greater than a certain threshold. The rectified linear unit (ReLU) will return 0 if the output is 0 or less or else itâ€™ll just return the output.</p>
<p>And yeah! Thatâ€™s the overall GIST of Deep Learning! This is like the 100th time I&#39;m saying this but do take a look at the Textbook article; youâ€™ll learn a lot more there!</p>

<h2>Getting into the Fun Stuff: Convolutional Neural Networks (CNNs)</h2>
<p>ahhhhh, Convolutional Neural Networks. this is where the real fun beings. no, not fun as in watching your model training for hours, but fun as in spending hours and hours on stackoverflow over shape errors ğŸ˜. Anyway, youâ€™re not here to hear me rant, so letâ€™s dive right into it!</p>
<p>CNNs are a type of deep learning algorithm that specializes in image and object recognition. Most people will usually say, â€œSri, whatâ€™s the difference between a CNN and a regular feedforward neural network?â€ Well, hereâ€™s the answer: while feedforward networks would work for basic images such as the MNIST dataset, theyâ€™ll be absolutely TERRIBLE at dealing with images that are so rich in data and pixels.</p>
<p>The overall goal of CNNs is to reduce the amount of â€œrichnessâ€ the image has into a form that is really easy to process while maintaining the main features and details that are required to produce a really good prediction.</p>
<h3>Chapter 1: Convolutions</h3>
<p>Convolutional neural network â€” convolutions = neural network. Get it? Without convolutions, CNNs simply donâ€™t exist. They â€œbreatheâ€ life into CNNs.</p>
<p>Convolutions work like this: we have something known as a kernel, a stride and padding. Letâ€™s try breaking this down. You have a really complex math problem that your teacher told you to get done by tomorrow:</p>
<img src="https://miro.medium.com/max/435/1*xVmQ6Vz0Hh4vFV-MjaYprA.png">
<p>Youâ€™re not a madman to just substitute whatever value the teacher told you to and just solve it out. Instead, you use your ğŸ§  and realize that, â€œHey! We can simply break this down until we get something really small thatâ€™s easier to deal with!â€</p>
<p>Thatâ€™s literally how convolutions work. The math problem is the image and the way you solve this is through your kernel, stride, and padding. Letâ€™s talk a bit more about each:</p>
<img src="https://miro.medium.com/max/700/0*iN6Fzjyx9PMWlHlL">
<p>The kernel (the green 3x3 grid above â˜) Is basically a filter that we pass through into our network. Using that, weâ€™d do matrix multiplication over the whole image. The outputs we get is what the red grid on the right is. The amount of distance the kernel moves is known as the stride. In the example above, the stride is set to one which simply means that it moves over to the next matrix of pixels one at a time. If you had a stride of two, thatâ€™d mean youâ€™d skip a row/column of pixels whenever doing this.</p>
<p>Once we do our matrix multiplication, weâ€™d sum everything up and then output our single value (the values in red). The kernel values (which does the matrix multiplication with the pixels) are usually chosen randomly and then optimized using gradient descent. But sometimes, your image matrixâ€™s length isnâ€™t even. What if you have a 5x5 matrix instead of the 6 by 6 above? Wouldnâ€™t the pixels overlap?</p>
<p>The answer to this is padding, specifically zero padding. The name gives it all; we add a border of zeros to the image.</p>
<img src="https://miro.medium.com/max/396/0*saQOlhM9vuTpZrIa">
<p>Now, after doing this, our matrix will become even again!! Hopefully that was an intuitive example of what convolutions are. If you have any questions, dm me <a href="https://twitter.com/srianumakonda">@srianumakonda.</a></p>

<h3>Chapter 2: Maxpooling</h3>
<p>Maxpooling. hmmmm ğŸ¤” sounds like maximizing your time âŒš when you swim ğŸŠâ€â™€ï¸.</p>
<p>OK, enough jokes for today. Maxpooling is taking the maximum value given a certain kernel size. Letâ€™s go back to the convolution we performed above. Generally, after an image gets convoluted, most people will then take the max pool of it. Letâ€™s say that our kernel size was 2x2. Just like convolutions, weâ€™d apply it over the image. From there, the greatest value within the kernel frame would be added to the output. The image below should clear any of your questions up ğŸ‘‡.</p>
<img src="https://miro.medium.com/max/700/0*x7lIWGH8L1Ni1zB6.gif">

<h3>Chapter 3: Dropout and Overfitting</h3>
<p>Think of overfitting like this: letâ€™s say that your teacher told you to learn your multiplication tables so that you can do EQAO. So, as a good student, you memorize everything. But then, when you write EQAO, they ask you to â€œexplain your answerâ€ and how 8 * 8 = 64. Well, youâ€™re screwed. You just focused on memorizing and not understanding how 8 * 8 = 64.</p>
<p>Thatâ€™s what overfitting is in neural networks. The neural network memorizes all the training data and when it gets given a testing image, it just breaks.</p>
<p>The way we solve this is through dropout. Dropout is a method that randomly drops â€œneuronsâ€. This means that instead of having a fixed amount of neurons for each layer, weâ€™re gonna have a layer with a random amount of nodes instead of following the x amount of nodes we defined.</p>
<img src="https://miro.medium.com/max/600/0*6rbrnpmRAgChmxVH.gif">
<p>Dropout sounded weird to me at first. Whatâ€™s the point of turning off neurons? Wouldnâ€™t that make the model worse since weâ€™re gonna losing those details and features?</p>
<p>No. Thatâ€™s not gonna happen. Why? Letâ€™s say that weâ€™re dealing with the MNIST dataset which is literally just a library of handwritten digits.</p>
<img src="https://miro.medium.com/max/480/0*nvwCAataeXyhK4sH.jpeg">
<p>When we feed this into the neural network, what if the weights are greater on the black background than the white background? What if the model is paying more attention to the background and not the digit itself?</p>
<p>BOOM ğŸ’¥. Thatâ€™s where dropout comes in. By turning off those nodes, theyâ€™ll randomly reset and start focusing on the handwritten digit itself and not continue to play around with the background.</p>

<h3>Chapter 4: Batch Normalization</h3>
<p>Batch normalization is generally a topic most people donâ€™t know much about. If nothing makes sense, <a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank">this article</a> and <a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/#:~:text=Batch%20normalization%20is%20a%20technique,required%20to%20train%20deep%20networks." target="_blank">this article</a> should help you a bit better.</p>
<p>Whenever we have image data, the first thing most people will do is normalize the data. What that basically means is that whenever we have an image (letâ€™s take a grayscale image for this example), the pixel values are between 0 and 255 (0 being black 255 being white). This is a HUGE range for the computer since of course, the computer has to convert everything to binary and use more RAM. So, what we can do is change the range from 0 to 255 to 0â€“1. That&#39;ll make it so much easier on the computer and help yourself by not having to wait really long.</p>
<p>The same can be applied to neural networks. Even if we pass everything through our activation functions, at some point weâ€™ll end up having a really large value. Normalizing that can make it a lot easier on the model, speed it up by allowing us to use a higher learning rate, and can reduce overfitting in some situations.</p>

<h3>Chapter 5: Activation Functions</h3>
<p>There are a BUNCH of activation functions. I already talked about this above but the TLDR of it is that activation functions tell the neural network whether certain nodes should be â€œturned onâ€ or not.</p>
<p>The activation function that I used for my neural network is known as the Rectified Linear Unit (ReLU). ReLU is hands down the most common type of activation function used because a) it can be used almost anywhere in Deep Learning and b) itâ€™s actually really simple.</p>
<img src="https://miro.medium.com/max/600/0*YEUa_s13RQGSx6e5.gif">
<p>The first thing to note is that ReLU is non-linear in nature so fitting non-linear data through the activation function shouldnâ€™t be a problem. The thing with ReLU is that it can make the model a lot lighter. ReLU works like this: if the output of the neuron â‰¥ 0, then the neuron wonâ€™t be fired since it has a value of 0. Else, ReLU will simply return the output.</p>
<p>The benefit of this is that not many neurons will fire which is surprisingly really good. This is mainly because itâ€™ll make the model a lot lighter which in turn, will be able to train a lot more faster and use less RAM. Itâ€™s also not complicated at all and can generally be used in any situation.</p>

<h3>Chapter 6: Putting it all together</h3>
<p>Now that weâ€™ve talked about the main components in the neural network, letâ€™s put it all together! You can see the overall model architecture below:</p>
<code>neural_network( <br>
    (layer1): Sequential(<br>
      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))<br>
      (1): ReLU()<br>
      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br>
      (3): Dropout(p=0.2, inplace=False)<br>
      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)<br>
    )<br>
    (layer2): Sequential(<br>
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))<br>
      (1): ReLU()<br>
      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br>
      (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)<br>
      (4): Dropout(p=0.25, inplace=False)<br>
      (5): ReLU()<br>
    )<br>
    (layer3): Sequential(<br>
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))<br>
      (1): ReLU()<br>
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br>
      (3): Dropout(p=0.2, inplace=False)<br>
      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)<br>
    )<br>
    (layer4): Sequential(<br>
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))<br>
      (1): ReLU()<br>
      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br>
      (3): Dropout(p=0.2, inplace=False)<br>
      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)<br>
    )<br>
    (flatten): Flatten()<br>
    (dropout): Dropout(p=0.25, inplace=False)<br>
    (output): Linear(in_features=512, out_features=2, bias=True)<br>
  )<br>
</code>
<p>We have 4 layers with each layer having a convolution, ReLU, then a BatchNormalization, Dropout, and then Maxpooling. We repeat that 4 times and then flatten the output into a 1-D vector. From there, we put another Dropout layer and then put a dense layer as our output (from 512 nodes to 2).</p>

<h2>Wrapping it all up: training and testing our model</h2>
<p>After testing and everything, the model got around 50â€“70% accuracy. Itâ€™s pretty bad but currently working on it to get the accuracy to 80%. You can find the GitHub code <a href="https://github.com/srianumakonda/Predicting-the-race--gender--and-ethnicity-of-people" target="_blank">here</a>. The code is currently being updated so make sure that you look at my GitHub!</p>

<h2>Cya later alligator ğŸŠ</h2>

<p>Thanks for reading! Hope it was a good read! Feedback is always appreciated so please do send it!</p>
<p>Cya!</p>
<p>- Sri âœŒ</p>


</body>

</html>



<!--<a href="https://medium.com/analytics-vidhya/the-ultimate-ai-textbook-dc2cf5dfe755" target="_blank">Ultimate AI Textbook</a>-->